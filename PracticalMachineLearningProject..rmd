---
title: "PracticalMachineLearning"
author: "Kenisha Jn Baptiste"
date: "6/10/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction  

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement – a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it.  

In this project, we will use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants to predict the manner in which they did the exercise. 


## Data Preprocessing  
```{r cache=FALSE}
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
library(corrplot)
```

### Download the Data
```{r, cache = T}
trainUrl <-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
trainFile <- "./data/pml-training.csv"
testFile  <- "./data/pml-testing.csv"
if (!file.exists("./data")) {
  dir.create("./data")
}
if (!file.exists(trainFile)) {
  download.file(trainUrl, destfile=trainFile, method="curl")
}
if (!file.exists(testFile)) {
  download.file(testUrl, destfile=testFile, method="curl")
}
```  
# Read in the Data and Identify the NA's
``` {r, cache = T}
traindata <- read.csv("./data/pml-training.csv", na.strings = c("NA", "#DIV/0!", ""))
testdata  <- read.csv("./data/pml-testing.csv", na.strings = c("NA", "#DIV/0!", ""))
``` 

# Remove NA Columns for the Training and Testing Data
``` {r, cache = T}
comps <- complete.cases(t(traindata)) & complete.cases(t(testdata))
traindata <- traindata[,comps]
testdata  <- testdata[,comps]
``` 

# Remove Columns With Data That Isn't Useful
``` {r, cache = T}
traindata <- traindata[,-c(1,3,4,5,6,7)]
testdata <- testdata[,-c(1,3,4,5,6,7)]
``` 

# Data Splitting
``` {r, cache = T}
set.seed(111)
inTrain <- createDataPartition(traindata$classe, p=0.6, list=FALSE)
traindata2 <- traindata[inTrain,]
validation <- traindata[-inTrain,]
``` 

```{r, cache = T}
controlRf <- trainControl(method="cv", 5)
modelRf <- train(classe ~ ., data=traindata2, method="rf", trControl=controlRf, ntree=250)
modelRf
```
# The Results on the Training Set
``` {r, cache = T}
trainresults <- predict(modelRf, traindata2)
trainacc <- sum(trainresults==traindata2$classe)/length(trainresults)
paste("Accuracy on training set =",trainacc)
```


## [1] "Accuracy on training set = 1"

# The Results on the Validation Set
``` {r, cache = T}
validresults <- predict(modelRf, newdata=validation)
validacc <- sum(validresults==validation$classe)/length(validresults)
paste("Accuracy on validation set =",validacc)
```

# The Results on the Test Set
``` {r, cache = T}
testresults <- predict(modelRf, newdata=testdata)
print("Classifications on the test set:"); testresults
```
# My Write-Up

#### The original training and testing data has 160 variables. I first removed columns with NA entries, which brought the number of variables down to 60. Secondly, 6 additional variables which contained information that was not useful were removed. Aside from "user_name" and "classe/problem_id", all of the remaining variables appear to be measurements from the fitness device.

#### Thirdly, the training data was then split into two sets: “traindata2” for training the model (60%) and “validation” for validation of the model (40%). For puposes of reproducibility, the seed was set to 111. Fourthly, a random forest on “traindata2” was trained using the default parameters. I chose a random forest model because they tend to be very accurate and the data set was small enough that using a random forest was feasible.

#### The classes on “traindata2” was then predicted and an accuracy of 100% was found. I therefore used this model to predict the values on the “validation” set and found the accuracy to be 99.2%. Since this was the first model that I tried I expect the out of sample error to be around 99%.

#### Finally, I applied this model to the testing data set and submitted my answers, and it correctly identified all 20 cases.

## The End